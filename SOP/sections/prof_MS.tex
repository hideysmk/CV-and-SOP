To improve people’s social sensibility, 
I have worked on social signal processing (\acr{SSP}) under the advisement of Dr. \textbf{Jane Yung-Jen Hsu}. 
SSP is a domain focusing on interpreting tiny slices of behaviors into status of social interaction. 
When I pursuing master’s degree in Dept. of Computer Science and Information Engineering (\acr{CSIE}) at National Taiwan University (\acr{NTU}), 
I proposed a framework to improve the process of analyzing Conversational Engagement. 
Traditionally, social signals are analyzed directly from digital signal by using sophisticated mathematical models. 
A gap exists between the sensor data and the interpretation of status of social activities, 
since psychological theories are based on observable behaviors but not collected numerical data. 
Instead of interpreting the intangible social status directly, I proposed that humans’ observable behaviors, 
such as turn-taking and voiced laugh, should be first detected and analyzed, 
and from these behaviors we can interpret one’s social signals with more concrete basis from psychology and behavioral science. 
With Coupled Hidden Markov Model (CHMM) presenting temporal information, 
the feature of behaviors are used to train a model for assessing conversational engagement level. 
In the experiment, the machine-assessed engagement level have a 82\% accuracy as hand-labeled engagement level by experts. 
The early version of the work was accepted by AAAI Activity Context Representation (ACR) Workshop 2012. 
Being accepted by the top research community, I first realized that I am qualified to deliver concrete research results. 
The chance of presenting and discussing the research idea to the top AI researchers also expands my view to AI and inspired me to conduct the following research, which led to the final version of my master’s thesis.

Audio channel is a possible approach to augment expressing social status, which I have explored under the supervision of Prof. Ted Selker. In 2012 summer, Prof. Selker recruited five students from NTU-CSIE to find possibility of further collaboration. I was the only student selected to join the CAMEO team, working on an intelligent online conference system. We added audio notification to present three interaction status: speaker identity, presence, and entry/exit. To fit the scenario, the audio cues are chosen according to their syntax (sound and placement) and semantic (relationship to the conversational channel). I then take the responsibility to build the website for experiment and analyzed the collected data. Successfully showing that audio cues can help user understand the interaction status, the paper was accepted by 14th IFIP-TC13 conference on HCI (INTERACT 2013). In addition to the achievement of research, the experience also helped me develop the ability of overcoming the challenge of facing new research topic with new colleagues in a new environment. After the CMUSV days, I am more confident that pursuing PhD is not only necessary but also a correct choice to my life.





